{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_dataset(df_all_path='save/df_all.pkl',\n",
    "                 id_test_path='save/id_test.pkl',\n",
    "                 target_path='save/target.pkl'):\n",
    "    \"\"\"\n",
    "    Load the data computed and saved by process_raw_data function.\n",
    "    \"\"\"\n",
    "    df_all = pickle.load(open(df_all_path, 'rb'))\n",
    "    id_test = pickle.load(open(id_test_path, 'rb'))\n",
    "    target = pickle.load(open(target_path, 'rb'))\n",
    "    print df_all.shape\n",
    "return df_all, id_test, target\n",
    "\n",
    "def split_train_valid_test(df_all, target, \n",
    "                           save_path='save/train_valid_test.pkl',\n",
    "                           random_state=0):\n",
    "    \"\"\"\n",
    "    This function split the data into: (X_train, y_train) + (X_valid, y_valid) \n",
    "    + (X_test, ). This splitting of the data allows the two level classification\n",
    "    approach used here (stacking of classifiers). \n",
    "    \n",
    "    - 1st level: Every classifiers is applied twice:\n",
    "        -First: The classifier is trained on (X_train, y_train) and tested on \\\n",
    "                (X_valid, y_valid). The prediction is stored in a the folder \\\n",
    "                'save/valid'.\n",
    "                \n",
    "        -Second: The classifier is trained on (X, y) = (X_train + X_valid, y_train + y_valid) \\\n",
    "                and test on (X_test,). The prediction is stored in the folder \\\n",
    "                'save/test'\n",
    "    - 2nd level: A classifier is trained with all solutions in the save/valid \\\n",
    "                folder and tested on the solutions in save/test. The prediction \\\n",
    "                is submitted. \n",
    "                \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df_all: pandas dataframe\n",
    "            The dataframe containing all data.\n",
    "    target: pandas dataframe\n",
    "            Labels of the training set\n",
    "    save_path: string\n",
    "            Path to the location where the training, validadion and test sets \n",
    "            will be stored.\n",
    "    random_state: numpy RandomState\n",
    "                 Used for reproducibility.\n",
    "                 \n",
    "    Return:\n",
    "    ------\n",
    "    X_train: numpy ndarray shape=(n_samples_train, n_features)\n",
    "            Training set\n",
    "    y_train: numpy array shape=(n_samples_train, )\n",
    "            Labels of training set \n",
    "    X_valid: numpy ndarray shape=(n_samples_validation, n_features)\n",
    "            Valiation set\n",
    "    y_valid: numpy ndarray shape=(n_samples_validation, )\n",
    "            Labels of validation set\n",
    "    X_test: numpy ndarray shape=(n_samples_test, n_features)\n",
    "            Test set\n",
    "    le: sklearn.preprocessing.LabelEncoder object\n",
    "        The label encoder object that is used to map original targets, i.e.\n",
    "        country name, to interger labels (from 0 to 11). This object is used\n",
    "        to back transform interger labels into the correct country name. \n",
    "    \"\"\"\n",
    "    #Spliting training and test data\n",
    "    piv_train = len(target) #Marker to split df_all into train + test\n",
    "    vals = df_all.values\n",
    "    X = vals[:piv_train]\n",
    "    X_test = vals[piv_train:]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(target.values)\n",
    "    \n",
    "    #The original training set is split into X_train (80%) + X_valid (20%)\n",
    "    sss = StratifiedShuffleSplit(y, 1, test_size=0.2, random_state=random_state)\n",
    "    for id_train, id_valid in sss:\n",
    "        X_train, X_valid = X[id_train], X[id_valid]\n",
    "        y_train, y_valid = y[id_train], y[id_valid]      \n",
    "    \n",
    "    #The label encoder is also saved to allow the inverse transform of labels. \n",
    "    pickle.dump([X_train,y_train,X_valid,y_valid,X_test, le], open(save_path,'w'))\n",
    "    \n",
    "return X_train, y_train, X_valid, y_valid, X_test, le\n",
    "\n",
    "\n",
    "\n",
    "def xgb_feat_selection(X_train, y_train, X_valid, y_valid, random_state):\n",
    "    \"\"\"\n",
    "    Feature selection based on the scores given to the features by an \n",
    "    XGB Classifier.\n",
    "    \"\"\"\n",
    "    #Parameters of the xgb classifier to be used for feature selection\n",
    "    params = {'eta': 0.09,\n",
    "              'max_depth': 6,\n",
    "              'subsample': 0.5,\n",
    "              'colsample_bytree': 0.5,\n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'num_class': 12}\n",
    "    num_rounds = 1000\n",
    "    xg_train = xgboost.DMatrix(X_train, label=y_train)  \n",
    "    xg_valid = xgboost.DMatrix(X_valid, label=y_valid)  \n",
    "    watchlist = [(xg_train,'train'), (xg_valid, 'validation')]\n",
    "    #Training the model and stopping at the best iteration\n",
    "    xgb = xgboost.train(params, xg_train, num_rounds, watchlist,\n",
    "                        early_stopping_rounds=10)\n",
    "    #Getting the scores for each feature\n",
    "    f_score = xgb.get_fscore()\n",
    "    feats = np.zeros(X_train.shape[1])\n",
    "    #Scores are given in the format => fn:x meaning n-th feature has a value x.\n",
    "    for k,v in f_score.items():\n",
    "        feats[int(k[1:])] = v\n",
    "    #Normalizing the scores to [0,1.]\n",
    "    feats = feats/float(np.max(feats))\n",
    "    \n",
    "    np.save('save/feat_sel_xgb.npy', feats)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def clf_keras(data, cl_weight=None, random_state=0, ext_name=\"\"):\n",
    "    \"\"\"\n",
    "    Keras MLP classifier.\n",
    "    The function applies the classifier twice:\n",
    "    - First: Fit the classifier to (X_train, y_train) and predict on (X_valid).\n",
    "             The prediction is stored in 'save/valid' folder.\n",
    "    - Second: Fit the classifier to (X, y) = (X_train + X_valid, y_train + y_valid)\n",
    "             and predict on (X_test). The prediction is stored in 'save/test' \n",
    "             folder.\n",
    "             \n",
    "    Parameters:\n",
    "    ----------\n",
    "    data: list\n",
    "         [X_train, y_train, X_valid, y_valid, X_test]\n",
    "    cl_weight: None or Dictionary\n",
    "         Class weights, e.g. {0:1, 1:1.5, 2:1.6...} => weight for class 0 is 1, \n",
    "         for class 1 is 1.5, for class 2 is 1.6, and so on.\n",
    "    random_state: numpy RandomState\n",
    "         RandomState used for reproducibility\n",
    "    ext_name: string\n",
    "         Extra string to be used in the name of the stored prediction, e.g. it \n",
    "         can be used to identify specific parameter values that were used.\n",
    "         \n",
    "    Result:\n",
    "    ------\n",
    "    y_valid_pred: numpy ndarray shape=(n_samples_validation, n_classes)\n",
    "              Labels of the predictions for the validation set.\n",
    "    y_test_pred: numpy ndarray shape=(n_samples_test, n_classes)\n",
    "              Labels of the predictions for the test set.\n",
    "              \n",
    "    Save:\n",
    "    ----\n",
    "    y_valid_pred: it is stored in save/valid folder\n",
    "    y_test_pred: it is stored in save/test folder \n",
    "    \"\"\"    \n",
    "    X_train, y_train, X_valid, y_valid, X_test = data  \n",
    "    \n",
    "    ###Working on (X_Train => X_Valid)###\n",
    "    #Centering and scaling the data\n",
    "    ss = StandardScaler()\n",
    "    XX_train = ss.fit_transform(X_train)\n",
    "    XX_valid = ss.transform(X_valid)   \n",
    "    \n",
    "    #Computing binary labels (required by keras)\n",
    "    lb = LabelBinarizer()\n",
    "    yb_train = lb.fit_transform(y_train)\n",
    "    yb_valid = lb.transform(y_valid)\n",
    "    \n",
    "    #Defining the network\n",
    "    dims = XX_train.shape[1]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.15, input_shape=(dims,)))\n",
    "    model.add(Dense(input_dim=dims, output_dim=1000, init='glorot_normal'))\n",
    "    model.add(MyPReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))    \n",
    "    model.add(Dense(input_dim=1000, output_dim=650, init='glorot_normal'))\n",
    "    model.add(MyPReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))    \n",
    "    model.add(Dense(input_dim=650, output_dim=350, init='glorot_normal'))\n",
    "    model.add(MyPReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.15))   \n",
    "    model.add(Dense(input_dim=350, output_dim=n_classes, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    \n",
    "    #These two callback objects are used to stop the training at the best\n",
    "    #iteration based on validation log_loss and to save that model\n",
    "    es = MyEarlyStopping(monitor='val_loss', patience=30, verbose=1)\n",
    "    mch = ModelCheckpoint('save/aux_keras_model', monitor='val_loss', \n",
    "                          save_best_only=True)\n",
    "    #Training the model\n",
    "    if cl_weight == None:\n",
    "        model.fit(XX_train, yb_train, nb_epoch=1000, batch_size=512, \n",
    "                  validation_data=(XX_valid, yb_valid), verbose=2,\n",
    "                  callbacks=[mch,es])\n",
    "    else:\n",
    "        model.fit(XX_train, yb_train, nb_epoch=1000, batch_size=512, \n",
    "                  validation_data=(XX_valid, yb_valid), verbose=2,\n",
    "                  callbacks=[mch,es], class_weight=cl_weight)        \n",
    "    #Loading the weights of the best epoch        \n",
    "    model.load_weights('save/aux_keras_model')\n",
    "    print es.best_epoch\n",
    "    #Predicting the labels of X_valid\n",
    "    y_valid_pred = model.predict_proba(XX_valid, batch_size=512, verbose=2)\n",
    "    #Computing the scores\n",
    "    ndcg_ke = np.mean([ndcg_score(tr, pr, k=5) for tr, pr in \\\n",
    "    zip(yb_valid.tolist(), y_valid_pred.tolist())])\n",
    "    logloss_ke = log_loss(y_valid, y_valid_pred)\n",
    "    print ndcg_ke, logloss_ke\n",
    "    \n",
    "    #Saving the result\n",
    "    rnd = random_state.randint(1000, 9999)\n",
    "    pickle.dump(y_valid_pred, open('save/valid/v_KE_%s_%s_%s_%s'%(ext_name, \n",
    "                rnd, round(ndcg_ke, 4), round(logloss_ke, 4)), 'w'))\n",
    "    \n",
    "return y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=6, releaselevel='final', serial=0)\n",
      "0 135483\n",
      "10000 135483\n",
      "20000 135483\n",
      "30000 135483\n",
      "40000 135483\n",
      "50000 135483\n",
      "60000 135483\n",
      "70000 135483\n",
      "80000 135483\n",
      "90000 135483\n",
      "100000 135483\n",
      "110000 135483\n",
      "120000 135483\n",
      "130000 135483\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44c84154ca1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-44c84154ca1c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mprocess_raw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-44c84154ca1c>\u001b[0m in \u001b[0;36mprocess_raw_data\u001b[0;34m(train_users_path, test_users_path, sessions_path)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mcont\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;31m#Creating a dataframe with the computed features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mcol_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#name of the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "def process_raw_data(train_users_path='../data/train_users.csv',\n",
    "                     test_users_path='../data/test_users.csv',\n",
    "                     sessions_path='../data/sessions.csv'):\n",
    "    \"\"\"\n",
    "    This function loads original data files, do all the feature engineering\n",
    "    and saves the necessary infomation for further processing.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    train_users_path: string\n",
    "        Path to train_users.csv file.\n",
    "    test_users_path: string\n",
    "        Path to test_users.csv file.\n",
    "    sessions_path: string\n",
    "        Path to sessions.csv file.\n",
    "        \n",
    "    Note: age_gender_bkts.csv and countries.csv files are not used.\n",
    "    \n",
    "    Save: This code process these files and saves\n",
    "    ----  \n",
    "    df_all.pkl: A pandas dataframe with all the data (traning + test sets).\n",
    "    id_test.pkl: IDs for test data. It will be needed to create a submission file. \n",
    "    target.pkl: Labels of the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    #########Loading data#############\n",
    "    #train_users\n",
    "    df_train = pd.read_csv(train_users_path)\n",
    "    target = df_train['country_destination']\n",
    "    df_train = df_train.drop(['country_destination'], axis=1)\n",
    "    \n",
    "    #test_users\n",
    "    df_test = pd.read_csv(test_users_path)    \n",
    "    id_test = df_test['id']\n",
    "\n",
    "    #sessions\n",
    "    df_sessions = pd.read_csv(sessions_path)\n",
    "    df_sessions['id'] = df_sessions['user_id']\n",
    "    df_sessions = df_sessions.drop(['user_id'],axis=1)\n",
    "    \n",
    "    #I am not using: age_gender_bkts.csv, countries.csv\n",
    "    \n",
    "    #########Preparing Session data########\n",
    "    #Filling nan with specific value ('NAN')\n",
    "    df_sessions.action = df_sessions.action.fillna('NAN')\n",
    "    df_sessions.action_type = df_sessions.action_type.fillna('NAN')\n",
    "    df_sessions.action_detail = df_sessions.action_detail.fillna('NAN')\n",
    "    df_sessions.device_type = df_sessions.device_type.fillna('NAN')\n",
    "    \n",
    "    #Action values with low frequency are changed to 'OTHER'\n",
    "    act_freq = 100  #Threshold for frequency\n",
    "    act = dict(zip(*np.unique(df_sessions.action, return_counts=True)))\n",
    "    df_sessions.action = df_sessions.action.apply(lambda x: 'OTHER' if act[x] < act_freq else x)\n",
    "\n",
    "    #Computing value_counts. These are going to be used in the one-hot encoding\n",
    "    #based feature generation (following loop).\n",
    "    f_act = df_sessions.action.value_counts().argsort()\n",
    "    f_act_detail = df_sessions.action_detail.value_counts().argsort()\n",
    "    f_act_type = df_sessions.action_type.value_counts().argsort()\n",
    "    f_dev_type = df_sessions.device_type.value_counts().argsort()\n",
    "    \n",
    "    #grouping session by id. We will compute features from all rows with the same id.\n",
    "    dgr_sess = df_sessions.groupby(['id'])\n",
    "    \n",
    "    #Loop on dgr_sess to create all the features.\n",
    "    samples = []\n",
    "    cont = 0\n",
    "    ln = len(dgr_sess)\n",
    "    for g in dgr_sess:\n",
    "        if cont%10000 == 0:\n",
    "            print(cont, ln)\n",
    "        gr = g[1]\n",
    "        l = []\n",
    "        \n",
    "        #the id\n",
    "        l.append(g[0])\n",
    "        \n",
    "        #The actual first feature is the number of values.\n",
    "        l.append(len(gr))\n",
    "        \n",
    "        sev = gr.secs_elapsed.fillna(0).values   #These values are used later.\n",
    "        \n",
    "        #action features\n",
    "        #(how many times each value occurs, numb of unique values, mean and std)\n",
    "        c_act = [0] * len(f_act)\n",
    "        for i,v in enumerate(gr.action.values):\n",
    "            c_act[f_act[v]] += 1\n",
    "        _, c_act_uqc = np.unique(gr.action.values, return_counts=True)\n",
    "        c_act += [len(c_act_uqc), np.mean(c_act_uqc), np.std(c_act_uqc)]\n",
    "        l = l + c_act\n",
    "        \n",
    "        #action_detail features\n",
    "        #(how many times each value occurs, numb of unique values, mean and std)\n",
    "        c_act_detail = [0] * len(f_act_detail)\n",
    "        for i,v in enumerate(gr.action_detail.values):\n",
    "            c_act_detail[f_act_detail[v]] += 1 \n",
    "        _, c_act_det_uqc = np.unique(gr.action_detail.values, return_counts=True)\n",
    "        c_act_detail += [len(c_act_det_uqc), np.mean(c_act_det_uqc), np.std(c_act_det_uqc)]\n",
    "        l = l + c_act_detail\n",
    "        \n",
    "        #action_type features\n",
    "        #(how many times each value occurs, numb of unique values, mean and std\n",
    "        #+ log of the sum of secs_elapsed for each value)\n",
    "        l_act_type = [0] * len(f_act_type)\n",
    "        c_act_type = [0] * len(f_act_type)\n",
    "        for i,v in enumerate(gr.action_type.values):\n",
    "            l_act_type[f_act_type[v]] += sev[i]   \n",
    "            c_act_type[f_act_type[v]] += 1  \n",
    "        l_act_type = np.log(1 + np.array(l_act_type)).tolist()\n",
    "        _, c_act_type_uqc = np.unique(gr.action_type.values, return_counts=True)\n",
    "        c_act_type += [len(c_act_type_uqc), np.mean(c_act_type_uqc), np.std(c_act_type_uqc)]\n",
    "        l = l + c_act_type + l_act_type    \n",
    "        \n",
    "        #device_type features\n",
    "        #(how many times each value occurs, numb of unique values, mean and std)\n",
    "        c_dev_type  = [0] * len(f_dev_type)\n",
    "        for i,v in enumerate(gr.device_type .values):\n",
    "            c_dev_type[f_dev_type[v]] += 1 \n",
    "        c_dev_type.append(len(np.unique(gr.device_type.values)))\n",
    "        _, c_dev_type_uqc = np.unique(gr.device_type.values, return_counts=True)\n",
    "        c_dev_type += [len(c_dev_type_uqc), np.mean(c_dev_type_uqc), np.std(c_dev_type_uqc)]        \n",
    "        l = l + c_dev_type    \n",
    "        \n",
    "        #secs_elapsed features        \n",
    "        l_secs = [0] * 5 \n",
    "        l_log = [0] * 15\n",
    "        if len(sev) > 0:\n",
    "            #Simple statistics about the secs_elapsed values.\n",
    "            l_secs[0] = np.log(1 + np.sum(sev))\n",
    "            l_secs[1] = np.log(1 + np.mean(sev)) \n",
    "            l_secs[2] = np.log(1 + np.std(sev))\n",
    "            l_secs[3] = np.log(1 + np.median(sev))\n",
    "            l_secs[4] = l_secs[0] / float(l[1])\n",
    "            \n",
    "            #Values are grouped in 15 intervals. Compute the number of values\n",
    "            #in each interval.\n",
    "            log_sev = np.log(1 + sev).astype(int)\n",
    "            l_log = np.bincount(log_sev, minlength=15).tolist()                      \n",
    "        l = l + l_secs + l_log\n",
    "        \n",
    "        #The list l has the feature values of one sample.\n",
    "        samples.append(l)\n",
    "        cont += 1\n",
    "    print(samples.shape)\n",
    "    #Creating a dataframe with the computed features    \n",
    "    col_names = []    #name of the columns\n",
    "    for i in range(len(samples[0])-1):\n",
    "        col_names.append('c_' + str(i)) \n",
    "    #preparing objects    \n",
    "    samples = np.array(samples)\n",
    "    samp_ar = samples[:, 1:].astype(np.float16)\n",
    "    samp_id = samples[:, 0]   #The first element in obs is the id of the sample.\n",
    "    \n",
    "    #creating the dataframe        \n",
    "    df_agg_sess = pd.DataFrame(samp_ar, columns=col_names)\n",
    "    df_agg_sess['id'] = samp_id\n",
    "    df_agg_sess.index = df_agg_sess.id\n",
    "    \n",
    "    #########Working on train and test data#####################\n",
    "    #Concatenating df_train and df_test\n",
    "    print(\"test\")\n",
    "    df_tt = pd.concat((df_test,None), axis=0, ignore_index=True)\n",
    "    df_tt.index = df_tt.id\n",
    "    df_tt = df_tt.fillna(-1)  #Inputing this kind of missing value with -1 (missing values in train and test)\n",
    "    df_tt = df_tt.replace('-unknown-', -1) #-unknown is another way of missing value, then = -1.\n",
    "   \n",
    "    ########Creating features for train+test\n",
    "    #Removing date_first_booking\n",
    "    df_tt = df_tt.drop(['date_first_booking'], axis=1)\n",
    "    \n",
    "    #Number of nulls\n",
    "    df_tt['n_null'] = np.array([sum(r == -1) for r in df_tt.values])\n",
    "    \n",
    "    #date_account_created\n",
    "    #(Computing year, month, day, week_number, weekday)\n",
    "    dac = np.vstack(df_tt.date_account_created.astype(str).apply(lambda x: map(int, x.split('-'))).values)\n",
    "    df_tt['dac_y'] = dac[:,0]\n",
    "    df_tt['dac_m'] = dac[:,1]\n",
    "    df_tt['dac_d'] = dac[:,2]\n",
    "    dac_dates = [datetime(x[0],x[1],x[2]) for x in dac]\n",
    "    df_tt['dac_wn'] = np.array([d.isocalendar()[1] for d in dac_dates])\n",
    "    df_tt['dac_w'] = np.array([d.weekday() for d in dac_dates])\n",
    "    df_tt_wd = pd.get_dummies(df_tt.dac_w, prefix='dac_w')\n",
    "    df_tt = df_tt.drop(['date_account_created', 'dac_w'], axis=1)\n",
    "    df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "    \n",
    "    #timestamp_first_active\n",
    "    #(Computing year, month, day, hour, week_number, weekday)\n",
    "    tfa = np.vstack(df_tt.timestamp_first_active.astype(str).apply(lambda x: map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]])).values)\n",
    "    df_tt['tfa_y'] = tfa[:,0]\n",
    "    df_tt['tfa_m'] = tfa[:,1]\n",
    "    df_tt['tfa_d'] = tfa[:,2]\n",
    "    df_tt['tfa_h'] = tfa[:,3]\n",
    "    tfa_dates = [datetime(x[0],x[1],x[2],x[3],x[4],x[5]) for x in tfa]\n",
    "    df_tt['tfa_wn'] = np.array([d.isocalendar()[1] for d in tfa_dates])\n",
    "    df_tt['tfa_w'] = np.array([d.weekday() for d in tfa_dates])\n",
    "    df_tt_wd = pd.get_dummies(df_tt.tfa_w, prefix='tfa_w')\n",
    "    df_tt = df_tt.drop(['timestamp_first_active', 'tfa_w'], axis=1)\n",
    "    df_tt = pd.concat((df_tt, df_tt_wd), axis=1)\n",
    "    \n",
    "    #timespans between dates\n",
    "    #(Computing absolute number of seconds of difference between dates, sign of the difference)\n",
    "    df_tt['dac_tfa_secs'] = np.array([np.log(1+abs((dac_dates[i]-tfa_dates[i]).total_seconds())) for i in range(len(dac_dates))])\n",
    "    df_tt['sig_dac_tfa'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).total_seconds()) for i in range(len(dac_dates))])\n",
    "#    df_tt['dac_tfa_days'] = np.array([np.sign((dac_dates[i]-tfa_dates[i]).days) for i in range(len(dac_dates))])\n",
    "\n",
    "    #Comptute seasons from dates\n",
    "    #(Computing the season for the two dates)\n",
    "    Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "    seasons = [(0, (date(Y,  1,  1),  date(Y,  3, 20))),  #'winter'\n",
    "               (1, (date(Y,  3, 21),  date(Y,  6, 20))),  #'spring'\n",
    "               (2, (date(Y,  6, 21),  date(Y,  9, 22))),  #'summer'\n",
    "               (3, (date(Y,  9, 23),  date(Y, 12, 20))),  #'autumn'\n",
    "               (0, (date(Y, 12, 21),  date(Y, 12, 31)))]  #'winter'\n",
    "    def get_season(dt):\n",
    "        dt = dt.date()\n",
    "        dt = dt.replace(year=Y)\n",
    "        return next(season for season, (start, end) in seasons\n",
    "                    if start <= dt <= end)\n",
    "    df_tt['season_dac'] = np.array([get_season(dt) for dt in dac_dates])\n",
    "    df_tt['season_tfa'] = np.array([get_season(dt) for dt in tfa_dates])\n",
    "    #df_all['season_dfb'] = np.array([get_season(dt) for dt in dfb_dates])\n",
    "    \n",
    "    #Age\n",
    "    #(Keeping ages in 14 < age < 99 as OK and grouping others according different kinds of mistakes)\n",
    "    av = df_tt.age.values\n",
    "    av = np.where(np.logical_and(av<2000, av>1900), 2014-av, av) #This are birthdays instead of age (estimating age by doing 2014 - value)\n",
    "    av = np.where(np.logical_and(av<14, av>0), 4, av) #Using specific value=4 for age values below 14\n",
    "    av = np.where(np.logical_and(av<2016, av>2010), 9, av) #This is the current year insted of age (using specific value = 9)\n",
    "    av = np.where(av > 99, 110, av)  #Using specific value=110 for age values above 99\n",
    "    df_tt['age'] = av\n",
    "    \n",
    "    #AgeRange\n",
    "    #(One-hot encoding of the edge according these intervals)\n",
    "    interv =  [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 100]\n",
    "    def get_interv_value(age):\n",
    "        iv = 20\n",
    "        for i in range(len(interv)):\n",
    "            if age < interv[i]:\n",
    "                iv = i \n",
    "                break\n",
    "        return iv\n",
    "    df_tt['age_interv'] = df_tt.age.apply(lambda x: get_interv_value(x))\n",
    "    df_tt_ai = pd.get_dummies(df_tt.age_interv, prefix='age_interv')\n",
    "    df_tt = df_tt.drop(['age_interv'], axis=1)\n",
    "    df_tt = pd.concat((df_tt, df_tt_ai), axis=1)\n",
    "    \n",
    "    #One-hot-encoding features\n",
    "    ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "    for f in ohe_feats:\n",
    "        df_tt_dummy = pd.get_dummies(df_tt[f], prefix=f)\n",
    "        df_tt = df_tt.drop([f], axis=1)\n",
    "        df_tt = pd.concat((df_tt, df_tt_dummy), axis=1)    \n",
    "       \n",
    "    ######Merging train-test with session data#################\n",
    "    df_all = pd.merge(df_tt, df_agg_sess, how='left')\n",
    "    df_all = df_all.drop(['id'], axis=1)\n",
    "    df_all = df_all.fillna(-2)  #Missing features for samples without sesssion data.\n",
    "    \n",
    "    #All types of null \n",
    "    df_all['all_null'] = np.array([sum(r<0) for r in df_all.values])\n",
    "    \n",
    "    ######Saving dataframe#######\n",
    "    #(saving necessary data for further computation)\n",
    "    df_all.to_pickle('save/df_all.pkl')\n",
    "    pickle.dump(id_test, open('save/id_test.pkl', 'wb'))\n",
    "    pickle.dump(target, open('save/target.pkl', 'wb'))\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_dataset(df_all_path='save/df_all.pkl',\n",
    "                 id_test_path='save/id_test.pkl',\n",
    "                 target_path='save/target.pkl'):\n",
    "    \"\"\"\n",
    "    Load the data computed and saved by process_raw_data function.\n",
    "    \"\"\"\n",
    "    df_all = pickle.load(open(df_all_path, 'rb'))\n",
    "    id_test = pickle.load(open(id_test_path, 'rb'))\n",
    "    target = pickle.load(open(target_path, 'rb'))\n",
    "    print(df_all.shape)\n",
    "    return df_all, id_test, target\n",
    "    \n",
    "    \n",
    "    \n",
    "def split_train_valid_test(df_all, target, \n",
    "                           save_path='save/train_valid_test.pkl',\n",
    "                           random_state=0):\n",
    "    \"\"\"\n",
    "    This function split the data into: (X_train, y_train) + (X_valid, y_valid) \n",
    "    + (X_test, ). This splitting of the data allows the two level classification\n",
    "    approach used here (stacking of classifiers). \n",
    "    \n",
    "    - 1st level: Every classifiers is applied twice:\n",
    "        -First: The classifier is trained on (X_train, y_train) and tested on \\\n",
    "                (X_valid, y_valid). The prediction is stored in a the folder \\\n",
    "                'save/valid'.\n",
    "                \n",
    "        -Second: The classifier is trained on (X, y) = (X_train + X_valid, y_train + y_valid) \\\n",
    "                and test on (X_test,). The prediction is stored in the folder \\\n",
    "                'save/test'\n",
    "    - 2nd level: A classifier is trained with all solutions in the save/valid \\\n",
    "                folder and tested on the solutions in save/test. The prediction \\\n",
    "                is submitted. \n",
    "                \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df_all: pandas dataframe\n",
    "            The dataframe containing all data.\n",
    "    target: pandas dataframe\n",
    "            Labels of the training set\n",
    "    save_path: string\n",
    "            Path to the location where the training, validadion and test sets \n",
    "            will be stored.\n",
    "    random_state: numpy RandomState\n",
    "                 Used for reproducibility.\n",
    "                 \n",
    "    Return:\n",
    "    ------\n",
    "    X_train: numpy ndarray shape=(n_samples_train, n_features)\n",
    "            Training set\n",
    "    y_train: numpy array shape=(n_samples_train, )\n",
    "            Labels of training set \n",
    "    X_valid: numpy ndarray shape=(n_samples_validation, n_features)\n",
    "            Valiation set\n",
    "    y_valid: numpy ndarray shape=(n_samples_validation, )\n",
    "            Labels of validation set\n",
    "    X_test: numpy ndarray shape=(n_samples_test, n_features)\n",
    "            Test set\n",
    "    le: sklearn.preprocessing.LabelEncoder object\n",
    "        The label encoder object that is used to map original targets, i.e.\n",
    "        country name, to interger labels (from 0 to 11). This object is used\n",
    "        to back transform interger labels into the correct country name. \n",
    "    \"\"\"\n",
    "    #Spliting training and test data\n",
    "    piv_train = len(target) #Marker to split df_all into train + test\n",
    "    vals = df_all.values\n",
    "    X = vals[:piv_train]\n",
    "    X_test = vals[piv_train:]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(target.values)\n",
    "    \n",
    "    #The original training set is split into X_train (80%) + X_valid (20%)\n",
    "    sss = StratifiedShuffleSplit(y, 1, test_size=0.2, random_state=random_state)\n",
    "    for id_train, id_valid in sss:\n",
    "        X_train, X_valid = X[id_train], X[id_valid]\n",
    "        y_train, y_valid = y[id_train], y[id_valid]      \n",
    "    \n",
    "    #The label encoder is also saved to allow the inverse transform of labels. \n",
    "    pickle.dump([X_train,y_train,X_valid,y_valid,X_test, le], open(save_path,'w'))\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, le\n",
    "\n",
    "\n",
    "\n",
    "def load_train_valid_test(path='save/train_valid_test.pkl'):\n",
    "    \"\"\"\n",
    "    Loads (X_train, y_train, X_valid, y_valid, X_test, label_encoder)\n",
    "    \"\"\"\n",
    "    return pickle.load(open(path, 'r'))\n",
    "    \n",
    "    \n",
    "\n",
    "def make_submission(y_pred, le, id_test_path='save/id_test.pkl', \n",
    "                    sub_name='sub.csv'):\n",
    "    \"\"\"\n",
    "    Makes a submission given a prediction. Creates the file according to the\n",
    "    competition format for submission.\n",
    "    \n",
    "    Paramters:\n",
    "    ---------\n",
    "    y_pred: numpy ndarray shape(n_samples_test,)\n",
    "            Prediction to be submitted.\n",
    "    le: sklearn.preprocessing.LabelEncoder object\n",
    "        The label encoder object that is used to map original targets.\n",
    "    id_test_path: string\n",
    "           Path to the id_test file.\n",
    "    sub_name: string\n",
    "        Name (path) of the submission file to be created.\n",
    "    \"\"\"\n",
    "    id_test = pickle.load(open(id_test_path, 'rb'))\n",
    "    ids = []\n",
    "    cts = []\n",
    "    nc = 5\n",
    "    #Taking the 5 classes with highest probabilities.\n",
    "    for i in range(len(id_test)):\n",
    "        idx = id_test[i]\n",
    "        ids += [idx]*nc\n",
    "        cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:nc].tolist()\n",
    "    \n",
    "    sample_submission = {}\n",
    "    sample_submission['id'] = ids\n",
    "    sample_submission['country'] = cts\n",
    "    #Creating a pandas dataframe with the submission.\n",
    "    s = pd.DataFrame.from_dict(sample_submission)\n",
    "    s.to_csv(sub_name, index=False)\n",
    "def main():\n",
    "    process_raw_data()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('64bit', 'ELF')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
